{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the basic elements of optimization? Give some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Optimization:__\n",
    "    Choosing Inputs that results in best possible output the best they can be.\n",
    "\n",
    "__Basic Elements of Optimization:__\n",
    "\n",
    "__Objective Function:__\n",
    " It is a value we are trying to optimize. \n",
    " \n",
    " __Example:__ \n",
    "\n",
    "Trying to make a square as big as possible the area of the square is __area=a*b__ will be the objective function.\n",
    " The main goal is trying to improve the value of objective function it means minimizing it,maximizing it or trying\n",
    " to bring it to certain value. Looking at the objective function value is one of the common ways to tell how well the optimization has been worked.In case where there are multiple objectives. they are summed (i.e) __objective=a+b__ , multiplied (i.e)__ojective=a*b__ or otherwise combined to form a single value (i.e) __objective=3a+4ab__. In dynamic optimization controk variables also form part of the objective function.It is commonly writtern in the form\n",
    " __minimize f(x)__ ,where f is objective function.The objective function the value that me and my optimization program are trying to optimize the objective function either minimized or maximized.\n",
    " \n",
    " __Decision Variables:__\n",
    " \n",
    " Decision variables are the input to the problem that your optimizer is allowed to cahnge to try to improve the objective function value.\n",
    "\n",
    " __Example:__ \n",
    "As square example above the decision variables would be the length of the two sides (i.e)(a,b) the variables are also called design variables or manipulated variables. As was stated before optimization problems are commonly writtern in form \n",
    "__minimize f(x)__ here x represents one or more decision variables in general the more decision variables in general the more decision variables there are the more difficult an optimization problem becomes to solve \n",
    "(i.e)__minimize f(x1+x2+x3+x4+x5...)__ .Desicion variables are the values that the optimization algorithm is allowed to choose or change.\n",
    " \n",
    " __Constraints__:\n",
    " Contraints defines where the optimizer  cant go or odditional conditions that must be met for a succesfful solution.\n",
    " \n",
    " __Example:__ \n",
    " When optimizing the size of the square we could ass the contraints that the length of two sides (i.e:a,b) multiplied together __a*b<=5__ this is an inequality constraint.We can also add equality constraints __a+b=10__ or __((ab)/(a+b))=3__, other example of constraints might be a bridge with a constraint that it must hold atleast 80,000 pounds or a chemical mixture that must be atleast 99% pure.Optimal solutions tend to be right up against the contraints (i.e) Picture a ball rolling downhill and encountring a wall.Constraints tell the software where it cannot go they often represents physical limitations of a system\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Why is optimization important in machine learning? Give some examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance of Optimization in Machine Learning\n",
    "Importance of machine learning depends on the methods and techniques we use in machine learning.\n",
    "Optimization is very important in some methods but not in all the methods.\n",
    "Example: Hastie, Tibshivani and Friedman's elements of satistical learning is considered as the most important reference in machine learning and it has very little reference to optimization.\n",
    "But when it comes to developing efficient and scalable algorithms to solve the machine learning problems, then optimizatoin is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a loss function /cost function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cost function is a function of input prices and output quantity whose value is the cost of making that output given those input prices, often applied through the use of the cost curve by companies to minimize cost and maximize production efficiency. There are a variety of different applications to this cost curve which include the evaluation of marginal costs and sunk costs. \n",
    "\n",
    "__Example__:\n",
    "The management of Duralex Companies, a manufacturer of toys, has asked for a new cost study to improve next yearâ€™s budget forecasts. They pay rent of $300 a month and they pay an average of $30 a month for electricity. Each toy requires $5 in plastic and $2 in cloth.\n",
    "\n",
    "A. How much will it cost them to manufacture 1200 toys annually?\n",
    "B. How much will it cost them to manufacture 1500 toys annually?\n",
    "\n",
    "First thing to do is to determine which costs are fixed and which ones are variable. Remember, fixed costs are incurred whether or not we manufacture, whereas variable costs are incurred per unit of production. That means rent and electricity are fixed while plastic and cloth are variable costs.\n",
    "\n",
    "Remember our cost function:\n",
    "C(x) = FC + V(x)\n",
    "Substitute the amounts.\n",
    "\n",
    "A. At 1200\n",
    "C(1,200) = $3,960* + 1,200 ($5 + $2)\n",
    "C(1,200) = $ 12,360\n",
    "\n",
    "Therefore, it would take $11,360 to produce 1,200 toys in a year.\n",
    "\n",
    "B. At 1500\n",
    "C(1,500) = $3,960* + 1,500 ($5 +$2)\n",
    "C(1500)= $14,460\n",
    "\n",
    "Therefore, it would take $13,460 to produce 1,500 toys in a year.\n",
    "\n",
    "*FC = (300 +30) * 12 months (remember we are asked at an annual basis).\n",
    "\n",
    "Thus, FC= $ 3,960\n",
    "\n",
    "(Notice that the fixed costs remain unchanged even at varying outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a gradient operator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient operator can be thought of as a generalization of the concept of a slope for multivariable functions. It is a vector that, given a scalar field, points to the directive of the steepest change in the function. Take for example the scalar function f(x,y,z). Its gradient is âˆ‡f=âˆ‚f/âˆ‚xx^+âˆ‚f/âˆ‚yy^+âˆ‚f/âˆ‚zz^\n",
    "\n",
    "We can express the change in the function as df=âˆ‡fâ‹…dr. We see that df is maximum the dr and âˆ‡f are parallel.\n",
    "Gradient operator is the first type of operators used for edge detection. The gradient of an image is a vector consisting of the first-order derivatives (including the magnitude and direction) of an image.The Gradient (also called the Hamilton operator) is a vector operator for any N-dimensional scalar.The gradient operator is a bit similar to the derivative, as it shows the variation of a scalar field in space. For instance, letâ€™s say you are considering a temperature field. Every point in space has its own temperature attached to it. Now the gradient operator associates to each point a vector, based on the variation of the temperature around it. More specifically, if you followed the gradient vector on a really short distance, the temperature would increase by the norm of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation between quadratic form, Hessian, Precision matrix, Mahalanobis distance and Optimization\n",
    "\n",
    "For a Gaussian random vector $\\mathbf{\\Theta}$ with mean $\\mathbf{\\Theta}^{*}$ and covariance matrix $\\mathbf{\\Sigma_{\\Theta}}$ and its joint PDF is given by\n",
    "\n",
    "![eq1](images/eq1.png)\n",
    "\n",
    "The objective function of the random vector can be defined as \n",
    "\n",
    "![eq2](images/eq2.png)\n",
    "\n",
    "We can see that the objective function of the random Gaussian vector is a quadratic function of the components in $\\mathbf{\\Theta}$. About the mean $\\mathbf{\\Theta}^{*}$, the $(l,l')$ component of the Hessian matrix can be written as:\n",
    "\n",
    "![eq3](images/eq3.png)\n",
    "\n",
    "So for a Gaussian random vector, the Hessian matrix evaluated at the mean is the invserse of the covariance matrix (which is the Precision matrix)\n",
    "\n",
    "### Hessian and Optimization\n",
    "A gradient descent with step size $\\alpha$ can be expressed as:\n",
    "\n",
    "$$x^{t+1} = x^t - \\alpha(Hx^{t} - b)$$\n",
    "\n",
    "Where $H$ is the Hessian matrix\n",
    "\n",
    "In the direction of the $i{th}$ eigen vector,\n",
    "\n",
    "$$x^{t+1} = (1 - \\alpha\\lambda_i)^tx^0$$\n",
    "\n",
    "where $x^0$ is the initial vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is Hessian related to the gradient, eigenvalues, optimization and convergence?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Definition__\n",
    "\n",
    "Let $f:Rnâ†’Rm$.\n",
    "\n",
    "We say, that f is differentiable at pointx0, if there exists a linear transformation A(x0), such that $f(x0+âˆ†x) =f(x0) +A(x0)âˆ†x+o(âˆ†x)$\n",
    "\n",
    "We call a function  f differentiable on a set $QâŠ‚Rn$,\n",
    "\n",
    "if it is differentiable at each point of Q. If  f  is differentiable on Rn, we just say that  f  is differentiable.\n",
    "\n",
    "The matrix A(x0)  is referred to as the  derivative  or  Jacobi matrix of   f(at pointx0),\n",
    "and is denoted by $âˆ‚f(x0)âˆ‚xorfâ€²(x0)$.\n",
    "\n",
    "Exercise:\n",
    "\n",
    "What doeso(âˆ†x) mean in the previous definition and why isnâ€™t ito(â€–âˆ†xâ€–)?\n",
    "\n",
    "Theorem 1.1$(A(x0))ij=âˆ‚fi(x0)âˆ‚xj$\n",
    "\n",
    "Exercise:\n",
    "\n",
    "First consider $f:Rnâ†’R$ and use the total differential formula $df(x) =âˆ‘ni=1âˆ‚fâˆ‚xidxi$\n",
    "\n",
    "Theorem 1.2\n",
    "$(f(g(x))â€²=fâ€²(g(x))gâ€²(x)$\n",
    "\n",
    "Exercise:\n",
    "\n",
    "First consider $f:Rnâ†’R$ and use the chain rule $âˆ‚fâ—¦gâˆ‚xj(x) =âˆ‘ni=1âˆ‚fâˆ‚xi(g(x))Â·âˆ‚giâˆ‚xj(x)$.\n",
    "\n",
    "Let $f:Rnâ†’R$ be differentiable. Its derivative (at pointx0) is then a $1Ã—nmatrixA(x0) =(âˆ‚f(x0)âˆ‚x1âˆ‚f(x0)âˆ‚x2...âˆ‚f(x0)âˆ‚xn)$\n",
    "\n",
    "The vectorA(x0)Tis referred to as the gradient of  f and is denoted as grad  $xf(x0) orâˆ‡xf(x0)$\n",
    "or simply $âˆ‡f(x0)$.\n",
    "\n",
    "Theorem 1.3\n",
    "$âˆ‡(f(x)g(x)) =âˆ‡f(x)g(x) +f(x)âˆ‡g(x)âˆ‡(f(x)/g(x)) = (âˆ‡f(x)g(x)âˆ’f(x)âˆ‡g(x))/g2(x)âˆ‡(f(g(x))) =âˆ‡f(g(x))âˆ‡g(x) =fâ€²(g(x))âˆ‡g(x)$\n",
    "\n",
    "__Hessian Definition__\n",
    "\n",
    "Let $f:Rnâ†’R$ be differentiable and $âˆ‡f:Rnâ†’Rn$  differentiable (i.e. let $f$ be twice differentiable).\n",
    "\n",
    "The derivative of$âˆ‡f$ at point$x0$ is $annÃ—nmatrixH(x0)$\n",
    "\n",
    "which is referred to as the second derivativeor Hessian of $f$(at pointx0) and is denoted  as\n",
    "$âˆ‚2f(x0)âˆ‚2xorâˆ‡2f(x0)$.\n",
    "\n",
    "Theorem 2.1\n",
    "$(H(x0))ij=âˆ‚2f(x0)âˆ‚xiâˆ‚xj$\n",
    "\n",
    "Exercise:\n",
    "Statement follows from (1.1).\n",
    "\n",
    "Theorem 2.2\n",
    "\n",
    "If partial derivatives $âˆ‚2fâˆ‚xiâˆ‚xjandâˆ‚2fâˆ‚xjâˆ‚xi$  of function $f$ are continuous at $x0$, then they are equal.\n",
    "Thus, Hessian of a sufficiently smooth function is a symmetric matrix.\n",
    "\n",
    "Theorem 2.3\n",
    "All eigenvalues of a symmetric matrix are real.\n",
    "\n",
    "Theorem 2.4\n",
    "The set of eigenvectors of a symmetric matrix contains an orthonormalbasis as a subset.\n",
    "\n",
    "Theorem 2.5\n",
    "$f(x0+âˆ†x) =f(x0) +âˆ‚f(x0)âˆ‚xâˆ†x+12âˆ†xT(âˆ‚2f(x0)âˆ‚2x)âˆ†x+o(â€–âˆ†xâ€–2)$\n",
    "\n",
    "\n",
    "__The relation between the Hessian and optimization__:\n",
    "\n",
    "The Hessian determines the curve of the loss function, so probably has some relation to how gradient descent would proceed. The key intuition is the relation between the eigenvalue, the eigenvector, and the speed and direction of convergence when using gradient descent. Long story short, the larger the eigenvalue, the faster the convergence from the direction of its corresponding eigenvector.\n",
    "\n",
    "Mathematically, this can be (casually) derived as follows:\n",
    "   \n",
    "   A gradient descent step with step size ð›¼ can be expressed as follows:\n",
    "   \n",
    "ð‘¥(ð‘¡+1)=ð‘¥ð‘¡âˆ’ð›¼(ð´ð‘¥(ð‘¡)âˆ’ð‘)\n",
    "\n",
    "   If ð‘¥(ð‘¡)=ð‘£ð‘–\n",
    "   \n",
    "   (the i -th eigenvector),\n",
    "   \n",
    "ð‘¥(ð‘¡+1)=(1âˆ’ð›¼ðœ†ð‘–)ð‘¡ð‘¥0\n",
    "\n",
    "where x_0 is the initial vector.\n",
    "\n",
    "   Any vector x can be expressed as a weighted sum of eigenvectors. Therefore, for each component, the convergence rate is 1âˆ’ð›¼ðœ†ð‘–\n",
    "   .\n",
    "Intuitively, this is because the curve is steeper in directions with larger eigenvalues. Imagine a ball rolling down the loss curve. Going back to our one-dimensional example,\n",
    "\n",
    "you can see that the stronger the curve, the quicker the ball reaches the minima, where the gradient is zero. You could also imagine how fast the gradient is converging to zero. Letâ€™s take the gradient of the two loss functions above and plot them.\n",
    "\n",
    "An astute reader will probably have realized, but there is no guarantee that the convergence rate 1âˆ’ð›¼ðœ†ð‘–\n",
    "is between 0 and 1. In fact, if we choose the wrong step size ð›¼, the parameters could start diverging. This is similar to how rolling a ball too quickly down the loss curve would cause it to fly off into the distance.\n",
    "\n",
    "\n",
    "The reason why a wrong step size could cause problems is that if the gradient changes rapidly, the gradient a few steps away would probably be completely different from its current value. We donâ€™t necessarily mind the gradient becoming slightly different. What concerns us is whether the sign of the gradient is the same. If we take a step that is too large, we could end up actually increasing the loss. The ideal step size for each eigenvector component is when x converges to 0 in one step. Therefore, it is ð›¼=1ðœ†ð‘–.\n",
    "\n",
    "If all the eigenvalues are the same, choosing the step-size is trivial. The problem is when the eigenvalues are very different. In this case, we have to be careful not to allow any of the components to diverge: this means that the step-size is effectively bounded by the largest curvature = largest eigenvalue. On the other hand, the speed of convergence is determined by the smallest eigenvalue. This is why the ratio between the largest and smallest eigenvalues in the Hessian is a very important value and has its own name: the â€œcondition numberâ€. The problem of the condition number being very large is called â€œill-conditioningâ€, and is a prevalent problem in many areas of optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Eigenvectors and Eigenvalues of the Hessian__\n",
    "\n",
    "Neural networks are far too complex to analyze, so we will build our intuition using the following, simple problem: minimizing a quadratic form. Quadratic forms are nothing to be afraid of; a two-dimensional quadratic form is simply any function that can be expressed like:\n",
    "\n",
    "$\\frac{1}{2}a_{11}x_1^2 + a_{12}x_1x_2 +\\frac{1}{2}a_{22}x_2^2 + b_1x_1 + b_2x_2 + c$\n",
    "\n",
    "When we write this in matrix form, we get the following equation\n",
    "\n",
    "$\\frac{1}{2}x^TAx - b^Tx + c$\n",
    "\n",
    "where A is symmetric (its i -th row and i -th column are the same) 1. Note that this can be extended easily to arbitrarily large dimensions, and we are just focusing on the two-dimensional case for clarity.\n",
    "For this function, the gradient is Ax - b and the Hessian is A .  If you know matrix calculus, this is easy to derive. Otherwise, you could try deriving the derivative for each element in x.  The optimal solution is where the gradient Ax - b becomes 0. Therefore, it is $x = A^{-1}b.$\n",
    "\n",
    "The quadratic form is key to understanding the Hessian and appears all over the place, so is worth looking into in depth.\n",
    "The curvature in the direction of the eigenvector is determined by the eigenvalue. If the eigenvalue is larger, there is a larger curvature, and if it positive, the curvature will be positive, and vice-versa.\n",
    "Letâ€™s dissect these properties one by one\n",
    ".\n",
    "Remember the definition of the Hessian? (Iâ€™ll repost it here so you donâ€™t have to scroll up).\n",
    "${\\mathbf H}={\\begin{bmatrix}{\\dfrac {\\partial ^{2}f}{\\partial x_{1}^{2}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{n}}}\\\\[2.2ex]{\\dfrac {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{1}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{2}^{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{n}}}\\\\[2.2ex]\\vdots &\\vdots &\\ddots &\\vdots \\\\[2.2ex]{\\dfrac {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{1}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{n}^{2}}}\\end{bmatrix}}$.\n",
    "Letâ€™s look at the first element in the second row,$ \\frac{\\partial^2f}{\\partial x_2 \\partial x_2}$ .\n",
    "This is the rate of change in direction $x_2$ of the gradient in direction $x_1$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is momentum and what problem is it solving? You may use this as an additional reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Momentum__:\n",
    "\n",
    "An object which is moving has momentum. The amount of momentum (p) possessed by the moving object is the product of mass (m) and velocity (v). In equation form:\n",
    "\n",
    "ð‘=ð‘šâ€¢ð‘£\n",
    "\n",
    "An equation such as the one above can be treated as a sort of recipe for problem-solving. Knowing the numerical values of all but one of the quantities in the equations allows one to calculate the final quantity in the equation. An equation can also be treated as a statement which describes qualitatively how one variable depends upon another. Two quantities in an equation could be thought of as being either directly proportional or inversely proportional. Momentum is directly proportional to both mass and velocity. \n",
    "\n",
    "A two-fold or three-fold increase in the mass (with the velocity held constant) will result in a two-fold or a three-fold increase in the amount of momentum possessed by the object. Similarly, a two-fold or three-fold increase in the velocity (with the mass held constant) will result in a two-fold or a three-fold increase in the amount of momentum possessed by the object. Thinking and reasoning proportionally about quantities allows you to predict how an alteration in one variable would effect another variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
